---
title: "Being right in the information era"
subtitle: "First draft for discussion"
author: "Greig Russell"
date: "`r Sys.Date()`"
output:
  tufte::tufte_handout:
    citation_package: natbib
    latex_engine: xelatex
  tufte::tufte_html: default
  tufte::tufte_book:
    citation_package: natbib
    latex_engine: xelatex
bibliography: Reference.bib
link-citations: yes
---



```{r setup, include=FALSE}
library(tufte)
# invalidate cache when the tufte version changes
knitr::opts_chunk$set(tidy = FALSE, cache.extra = packageVersion('tufte'))
options(htmltools.dir.version = FALSE)
```

\pagenumbering{arabic}

# Introduction

`r newthought('No question occurs without a context')` or an origin story. Typically, in philosophy, questions like "What is the answer to " are immediately followed by "how do I know that I am right"? This thesis will focus on the epistemology challenge of the later. This issue has been the focus of philosophers (Descartes) and non-philosophers (Trump) for many centuries.  

`r newthought('Descartes [1641]')` considered the question of the nature of his reality by denying the relevance of sense data and only believing in the reality of his cognition. Arguing that he could be asleep and in a dream without the ability to distinguish this state from waking and perceiving, so only his thoughts were undisputably real. This lead to the famous position of "I think therefore I am" as the basis for his philosophical meditations on the nature of his reality [Descartes1641].  

In Meditation four,`r margin_note('Descrates argues he is right when his beliefs and actions are aligned to an external source of truth, in his case the Judo-Christian God as perceived in the 15th Century')` @Descartes1641 uses as his truth determinant the stated existence of God, who inherently does not have properties which would render his _(sic)_  judgement open to any doubt. Descartes, on the other hand, states his imperfections as being the basis of his errors [Descartes1641].    

Error or falsity occurs when Descartes exercises his free will (actions) to move beyond his cognitive understanding of a given situation. Conversely, if Descartes confines his efforts to situations where he has full knowledge, then he is right as evidenced by his ability to "so see there the image of God [Descartes1641]."  

 `r newthought('Hume [1751]')` had the diametrically opposite perspective to that of Descartes. Primary sensory perceptions and the impressions generated from reflections on those perceptions are the basis of ideas [@Hume1751, Section II - Of the origin of ideas]. 

The association between ideas for Hume can be one of "Resemblance" or "Continuity" and most importantly "Cause and effect" [@Hume1751, Section III - Of the association of ideas]. The first two can never have truth validity associated with them. A Tuatara resembles a lizard; only it is not. All swans are not white, but all swans in Europe are white, so when a European is thinking of a swan they will recall only a white swan, but this does not prove all swans are white.

In Section IV `r margin_note('Hume argues for the primacy of the senses as the basis of ideas. For Hume being right is establishing a causative relationship is an internally consistent chain that invariably decsribes the connection and is based on other sense derived phenomea or basic matehmatical axioms')`- "Skeptical doubts concerning the operations of the understanding" @Hume1759 focuses on the _"is-ought"_ paradox he first raised in his "A Treatise on Human Understanding" [@Hume1739, p. 379]. Namely, because one thing **is** associated with another,  does not mean they **ought** to be associated with each other.  For Hume, such infinite skepticism will be addressed through human reason, which he divides into two categories; "Relations of ideas" and "Matters of fact" [-@Hume1751].  

The concept of "Relations of ideas" is obviously influenced by Newton's laws of Physics, where describing the causal relationship between two ideas is mathematically derived and underpinned by the self-evident core axioms [@Hume1751, Part 1, Section IV]. Where the force created in one billiard ball is transferred to another by the collision and described by the formula for Newton's second law;

$$Force = Mass. Acceleration$$

"Matters of fact" or cause and effect between two ideas have to transcend the temporal or spatial. The example used is free will, or human thought can control finger movement but not liver action, so free will cannot cause the finger to move as it is not necessary or sufficient. For Hume, finger movement derives from the nerves connecting the brain to the finger and arm muscles. Each component in this causal chain is a sensory derived idea [@Hume1751, Part 1, Section VII - Of the idea of necessary connection].  

In this Hume was describing a deterministic universe based on internal consistency and derived from sense based information. Excluded are concepts of cognition or external social sources of validity including theism. These exclusions were fundamental to @Descartes1641 formula for being correct.  

`r newthought("History plays cruel jests")` on the wise. @Hume1751 rejected probability as a sufficient basis to underpin determination of cause behind an effect. The relentless determinism which followed instead demonstrated that the universe was by nature inherently probabilistic. 

In the early 20th century `r margin_note('Statistical truth merges the concepts of an intrinsic casual relationship defined by mathematics derived from fundamental axioms')`, statistics and probability developed into cornerstones of modern science, so that now little-deemed science occurs without them, and statistical techniques have split into non-traditional subjects such literature through techniques like text analysis [@silge2016tidytext].

@fisher1925statistical describes how he saw statistics as being the application of applied mathematics to populations. In his work, this original concept had been extended to an understanding of variation and the "reduction of data" (@fisher1925statistical p. 1). His worked extended into the study of frequency distributions, which describe the spread of a characteristic within a population and enable the use of specific statistical models to explain that variation [@fisher1925statistical]. 

Described is also the work by Pearson on correlation or co-variance (@fisher1925statistical, p.6). Co variance describes how the stronger two properties of a population are related can be measured by how much they vary at the same time.  @student1908probable describes the differences between two means, where a mean is a measure of the central tendency within a population. 

As a result of this work, the truth or validity was about the properties of two related populations; whether the difference was mathematically higher than that expected by chance alone or conversely if these two populations varied were statistically co-variant then it could mean they were causally related. This new methodology not only suggested possible avenues for further study, but it also provided a vehicle to demonstrate which explanation is most likely [@field2012discovering].

`r newthought('The philosophy of science')` presents its student with a different aspect to the challenge of knowing if you are right.`r margin_note('For Popper being right was having a valid thory that now one had proved wrong')` As most scientific theories are wrong and only a stepping stone towards a deeper understanding of the underlying question. For @popper2005logic the answer was that scientific theories were right if no one has proved them wrong. The nuance of this seemingly self-evident statement was for a scientific theory to be a valid theory; it had to generate new hypotheses to test including at least one that if true would disprove the theory [@popper2005logic].

For @kuhn2012structure, Popper may have described a scientific theory and its exploration through progressive experimentation, but there was more to science. `r margin_note('Kuhn extended this theory of being right to include a social acceptance by the scientific community')` For Kuhn saw the process of science as being organised around a central paradigm (-@kuhn2012structure).  Where adoption of this theory was a social process, as was its abandonment. Where doubts about the incumbent rose with evidence that did falsify it, leading to the development of the new theory. Adoption of this new theory would be a social process across the scientific community as a whole [@kuhn2012structure].

#The information era
`r newthought('A necessary precursor')` to any information age was the development of the modern computer.`r margin_note('For Turing the computer assisted the expert, the source of truth and the validity of the answer was the user and not the computer')`  Turing's (1937) conceptualization has become the core design concept, of which everything since has only been an implementation.  The components of his machine were;

1. A paper tape, which is usually described as being infinite, which contains the a set of 1's and 0's. The initial state of this paper tape describes the input to the process, the final state is the output.
2. The head which reads the paper tape and consults the set of instructions as to what next steps to take.
3. A set of instructions, such as change the value of this cell, then move n places to the left.

For Turing the machine was mostly a means to an end as the focus on his work was solving the "Holting problem", namely would such machines always end, with the answer being "no they would not" [@turing1937computable].  

There are two key elements to the Turing machine. The first is that it has no concept of validity. The message on the tape is what it was, both initially and on successful termination of what we would now call the program. Instead it used identity as a substitute. I would argue this is not an oversight, but a manifestation of Turing's reality. He was the source of validity and truth. The computer assisted him in that process, but was not in itself ever meant to be a source of the truth.

![A slide rule](/home/greig/Philosophy/Question/Slide_rule.png)

Consider a more accessible example the slide rule, the pocket calculator for all scientists and mathematicians of the era. Usefully the manufacturer places a mark for $\pi$ at 3.14. Slide rule users of ear would all know the value of $\pi$ and this was only a convenience, not an indication of the truth as to the value of $\pi$. If the mark was found to be in the wrong place, it would not cause error as the source of validity was with the user. 

`r newthought('In contrast, Floridi (2004)')` adopts the opposite stance that technology is fundamentally trans-formative regarding both how but what humanity can study. `r margin_note('Information is a physical entity that underpins processes analgous to computers. Validity is a function of these processes not the information.')`Mirroring the new epoch is the emergence of a new area of philosophical study, the Philosophy of information. 

>"PI is a new philosophical discipline, concerned with (a) the critical investigation of the conceptual nature and basic principles of information, including its dynamics (especially computation and information flow), utilization, and sciences, and with (b) the elaboration of information-theoretic and computational methodologies and their application to philosophical problems."

`r quote_footer('--- Floridi (2004) p. 555')`

Floridi proceeds to discuss the critical unanswered questions within the Philosophy of Information, starting with the need to define what information is [@floridi2004open]. Floridi (2004) then defines information as being loosely being about reality, specifically; statements of reality, information about reality and information for reality [@floridi2004open]. Then proceeding to use this perspective as the basis for his six hypothesis to define information, with a particular focus on communication theory.  The communication theory approach is derived from the work of Claude Shannon's 1948 mathematical model describing the reliable transmission of data across then telegraph wires and now the fiber optic world wide web[@shannon1948mathematical]. Within this primarily communication model, the definition of information then is the number of different data points possible to encode within a set size for a given data packet [@shannon1948mathematical].  This perspective sees data as being a physical entity without inherent meaning, hence can be either be right or wrong. Indeed, information, encoded as quantum particles, is seen as the data used in galactic scale processes analogous to terrestrial computers which drive the known universe [@stonier2012information]. Right and wrong then are whether the process both holts and develops a solution that is capable of sustaining itself within a given broader context.  

Floridi, therefore, appears to be arguing that information as data underpins the computer-analogous process which creates an outcome and is irrelevant to the role or rightness of the answer to a given question [@floridi2004open].   

Hence the answer is directly dependent on data, so knowing the data means knowing the answer. Floridi (2004) uses DNA as an example of information for reality. Apparently, this matches his vision as within the DNA is the genetic code for the consequent human, who is manifest by biological processes. Except it does not, a person's phenotype does not equal their genotype, as some or many genes are not activated. So these latent genes stay hidden until manifest as a function of an external social process between humans, e.g. diet or smoking.  

Data science has a more mundane definition of data, information and knowledge.  

The data elements are the atomic elements within a set of observations, the relationship between which provides information. Information serves as the basis for action [@anderson2015creating]. For example, whether the mumps, measles and rubella vaccination (MMR) status and the presence or absence of autism are atomic level data. Information is the relationship between the two. Namely, MMR does not cause autism. Knowledge for the parent is to set their mind at rest from the fraudulent process claiming there was a link [@brown2012uk].  

`r newthought('@Cresswell1988 captures this more familiar sense')` of information and how ordinary people make sense of meaning, so knowing about the truth of a given statement. Adopting Chomsky's differentiation between linguistic performance and semantic competence allows @Cresswell1988 to focus on the later.  

Semantic competence`r margin_note('Validity is seen as being inherent to the semantic competence of a natural speaker considering the truth conditional of a claim in this and all possible worlds')` as seen in a natural speaker is an inherent property of language. The lowest common denominator of communication is speech, and the inability to speak excludes one from society more so than difficulties with reading or writing. Speech then is the most common way for information to be passed between individuals and for judging the validity of the content.

@Cresswell1988 focuses on the central role of truth-conditions in understanding semantic competence, specifically the many worlds variety. So the truth-conditional validity needs to be considered not only in the current world but also in hypothetical other possible worlds [@Cresswell1988].

The MMR example makes this formulation of validity easily accessible;

(@) A child received the MMR vaccination (_data_)
**and**
(@) Autism did not occur (_data_)
**hence**
(@) Immunisation is safe (_information_)
**and**
(@) I can _imagine_ a possible future world where my child gets the complications of Mumps, Measles or Rubella (_possible future state_)
**hence**
(@) I should give my child the MMR vaccination. (_knowledge_)

For @Cresswell1988 the process of a semantic understanding of the nature and validity and a claim is the same in the medieval marketplace as it is on today's internet. Yet there are fundamental differences in scale.

In the medieval marketplace I can only only talk or even hear a small number of people at a given time. The truth conditional nature of claims is either self evident (_"the harvest is bountiful"_) or irrelevant (_"Do you think the world is round?_"). I can only attend the market weekly or monthly.  The modern internet is a Turing machine; where others control the machine and the truth condition of data can be impossible to determine or is deliberately fraudulent. My social network of choice feeds to my smart watch 24 hours a day, and false narratives can be created to influence real world outcomes like national elections on a grand scale. 

#Summary
This is key argument. Assessing the validity of any claim has not inherently changed with the information age. The philosophical tools remain the same and understanding what it means to be right is a continuation of the past debate. 

What has changed is the scale of social interconnections that has engulfed the world and the abstraction of external reality, along with the deluge of random data points and information, spurious or otherwise. More work needs to occur in extending existing philosophical tools. There is no suggestion that philosophy of information is a new branch of philosophy, but it is an new lens on old debates.

\newpage
##Appendix 1: Prolog programming language
@colmerauer1996birth `r margin_note('Although logic is core to Prolog, the utility of Prolog in Logic is less certain. Currently this is a tool with potential but without purpose.')`describes both the birth and the intent of the Prolog computing language in the early 1970s.  Prolog's focus was to be in natural language processing, but with time Prolog has also developed to become a general purpose computing language (@liu2018logic). Its most common use remains in natural language processing and the development of expert systems including IBM Watson (@liu2018logic, p. 15). 

Prolog aims to enable the representation of knowledge through the used of formal predicate logic (@baral1994logic). Logic programming is unique among computing paradigms in that it includes the concepts of fact and validity of expressions as fundamental core constructs, inherent to the language itself (@colmerauer1996birth). 

To a logistician, Prolog is an idiosyncratic representation of the usual and commonplace. Consider modus pons;

$$p \supset q$$
$$p$$
**Hence**
$$q$$

is represented in Prolog as;

\begin{center}
p \newline
q :- p \newline
\end{center}

Which is to be read as p is true, hence q must be true given p is true.

So Prolog provides a vehicle for knowledge representation through the use of formal logic. What is less obvious is what it adds to a discussion of the logic itself and its underlying formal proof.

\newpage