---
title: "Being right in the information era"
subtitle: "First draft for discussion"
author: "Greig Russell"
date: "`r Sys.Date()`"
output:
  tufte::tufte_handout:
    citation_package: natbib
    latex_engine: xelatex
  tufte::tufte_html: default
  tufte::tufte_book:
    citation_package: natbib
    latex_engine: xelatex
bibliography: Reference.bib
link-citations: yes
---



```{r setup, include=FALSE}
library(tufte)
# invalidate cache when the tufte version changes
knitr::opts_chunk$set(tidy = FALSE, cache.extra = packageVersion('tufte'))
options(htmltools.dir.version = FALSE)
```

\pagenumbering{arabic}

# Introduction

`r newthought('No question occurs without a context')` or an origin story. Typically, in philosophy, questions like "What is the answer to " are immediately followed by "how do I know that I am right"? This thesis will focus on the epistemology challenge of the later. This issue of knowing if the answer to your question is right has been the focus of attention by philosophers (Descartes) and non-philosophers (Trump) for many centuries, so why should how philosophers approach change in the information era?   @floridi2004open would argue because the information era is inherently different, and hence being right in the information age requires a new paradigm. This thesis will argue that being right in the information era is not inherently different and only extensions of the existing long standing philosophical tools are required. Being lost in a badly maintained library is as bewildering as the internet is to the naive, yet the internet is just the largest - and most poorly curated - ever known. 

`r newthought('Descartes [1641]')` considered the question of the nature of his reality by denying the relevance of sense data and only believing in the reality of his cognition. Arguing that he could be asleep and in a dream without the ability to distinguish this state from waking and perceiving, so only his thoughts were undisputably real. This lead to the famous position of "I think therefore I am" as the basis for his philosophical meditations on the nature of his reality [Descartes1641].  

In Meditation four,`r margin_note('Descrates argues he is right when his beliefs and actions are aligned to an external source of truth, in his case the Judo-Christian God as perceived in the 15th Century')` @Descartes1641 uses as his truth determinant the stated existence of God, who inherently does not have properties which would render his _(sic)_  judgement open to any doubt. Descartes, on the other hand, states his imperfections as being the basis of his errors [Descartes1641].    

Error or falsity occurs when Descartes exercises his free will (actions) to move beyond his cognitive understanding of a given situation. Conversely, if Descartes confines his efforts to situations where he has full knowledge, then he is right as evidenced by his ability to "so see there the image of God [Descartes1641]."  

 `r newthought('Hume [1751]')` had the diametrically opposite perspective to that of Descartes. Primary sensory perceptions and the impressions generated from reflections on those perceptions are the basis of ideas [@Hume1751, Section II - Of the origin of ideas]. 

The association between ideas for Hume can be one of "Resemblance" or "Continuity" and most importantly "Cause and effect" [@Hume1751, Section III - Of the association of ideas]. The first two can never have truth validity associated with them. A Tuatara resembles a lizard; only it is not. All swans are not white, but all swans in Europe are white, so when a European is thinking of a swan they will recall only a white swan, but this does not prove all swans are white.

In Section IV `r margin_note('Hume argues for the primacy of the senses as the basis of ideas. For Hume being right is establishing a causative relationship with an internally consistent chain that invariably describes the connection and is based on other sense derived phenomena or basic mathematical axioms')`- "Skeptical doubts concerning the operations of the understanding" @Hume1759 focuses on the _"is-ought"_ paradox he first raised in his "A Treatise on Human Understanding" [@Hume1739, p. 379]. Namely, because one thing **is** associated with another,  does not mean they **ought** to be associated with each other.  For Hume, such infinite skepticism will be addressed through human reason, which he divides into two categories; "Relations of ideas" and "Matters of fact" [-@Hume1751].  

The concept of "Relations of ideas" is obviously influenced by Newton's laws of Physics, where describing the causal relationship between two ideas is mathematically derived and underpinned by the self-evident core axioms [@Hume1751, Part 1, Section IV]. Where the force created in one billiard ball is transferred to another by the collision and described by the formula for Newton's second law;

$$Force = Mass. Acceleration$$

"Matters of fact" or cause and effect between two ideas have to transcend the temporal or spatial. The example used is free will, or human thought can control finger movement but not deliver action, so free will cannot cause the finger to move as it is not necessary or sufficient. For Hume, finger movement derives from the nerves connecting the brain to the finger and arm muscles. Each component in this causal chain is a sensory derived idea [@Hume1751, Part 1, Section VII - Of the idea of necessary connection].  

In this Hume was describing a deterministic universe based on internal consistency and derived from sense based information. Excluded are concepts of cognition or external social sources of validity including theism. These exclusions were fundamental to @Descartes1641 formula for being correct.  

`r newthought("History plays cruel jests")` on the wise. @Hume1751 rejected probability as a sufficient basis to underpin determination of cause behind an effect. The relentless determinism which followed instead demonstrated that the universe was by nature inherently probabilistic. 

In the early 20th century `r margin_note('Statistical truth merges the concepts of an intrinsic casual relationship defined by mathematics derived from fundamental axioms. Being right was showing a difference that was unlikely to be explained by chance alone.')`, statistics and probability developed into cornerstones of modern science, so that now little-deemed science occurs without them, and statistical techniques have split into non-traditional subjects such literature through techniques like text analysis [@silge2016tidytext].

@fisher1925statistical describes how he saw statistics as being the application of applied mathematics to populations. In his work, this original concept had been extended to an understanding of variation and the "reduction of data" (@fisher1925statistical p. 1). His worked extended into the study of frequency distributions, which describe the spread of a characteristic within a population and enable the use of specific statistical models to explain that variation [@fisher1925statistical]. Described is also the work by Pearson on correlation or co-variance (@fisher1925statistical, p.6). Covariance describes how the stronger two properties of a population are related can be measured by how much they vary at the same time.  @student1908probable describes the differences between two means, where a mean is a measure of the central tendency within a population. 

The truth or validity of a causal claim was supported by showing there was a difference between two populations, than was greater than could be explained by chance alone. A variation on the theme was if two events linked through a causal model of explanation, occurred together over 95% of the time, then this provided compelling support for theory being right. This new methodology not only suggested possible avenues for further study, but it also provided a vehicle to demonstrate which explanation is most likely [@field2012discovering].

`r newthought('The philosophy of science')` presents its student with a different aspect to the challenge of knowing if you are right.`r margin_note('For Popper being right was having a valid theory that no one had proved wrong and generated new testable hypotheses')` As most scientific theories are wrong and only a stepping stone towards a deeper understanding of the underlying question. For @popper2005logic the answer was that scientific theories were right if no one has proved them wrong. The nuance of this seemingly self-evident statement was for a scientific theory to be a valid theory; it had to generate new hypotheses to test including at least one that if true would disprove the theory [@popper2005logic].

For @kuhn2012structure, Popper may have described a scientific theory and its exploration through progressive experimentation, but there was more to science. `r margin_note('Kuhn extended this theory of being right to include a social acceptance by the scientific community of the theory')` For Kuhn saw the process of science as being organised around a central paradigm (-@kuhn2012structure).  Where adoption of this theory was a social process, as was its abandonment. Where doubts about the incumbent rose with evidence that did falsify it, leading to the development of the new theory. Adoption of this new theory would be a social process across the scientific community as a whole [@kuhn2012structure].

#The information era
`r newthought('A necessary precursor')` to any information age was the development of the modern computer.`r margin_note('For Turing, the computer was only a tool that assisted the expert user, the source of truth and the validity of the answer was never the computer.')`  Turing's (1937) conceptualization has become the core design concept, of which everything since has only been an implementation of this vision.  The components of his machine were;

1. A paper tape, which is usually described as being infinite, which contains a list of 1's and 0's. The initial state of this paper tape describes the input to the process, the final state is the output.
2. The head which reads the paper tape and consults the set of instructions as to what next steps to take.
3. A set of instructions, such as change the value of this cell, then move n places to the left.

For Turing the machine was mostly a means to an end as the focus on his work was solving the "Holting problem", namely would such machines always end, with the answer being "no they would not" [@turing1937computable].  

There are two key elements to the Turing machine. The first is that it has no concept of validity. The message on the tape is what it was, both initially and on successful termination of what we would now call the program. Instead it used identity as a substitute. I would argue this is not an oversight, but a manifestation of Turing's reality. He was the source of validity and truth. The computer assisted him in that process, but was not in itself ever meant to be a source of the truth.

![A slide rule](/home/greig/Philosophy/Question/Slide_rule.png)

Consider a more accessible example the slide rule, the pocket calculator for all scientists and mathematicians of the era. Usefully the manufacturer places a mark for $\pi$ at 3.14. Slide rule users of the era would all know the value of $\pi$ and this mark was only a convenience, not an indication of the truth as to the value of $\pi$. If the mark was found to be in the wrong place, it would not cause any error as the source of validity was the user. 

`r newthought('In contrast, Floridi (2004)')` adopts the opposite stance that technology is fundamentally trans-formative regarding both how also but what humanity can study. `r margin_note('Information is seen as a physical entity that underpins physical processes making reality is analogous to computers. Validity is a function of these processes not the underlying raw material the information used.')`Mirroring the new epoch is the emergence of a new area of philosophical study, the Philosophy of information. 

>"PI is a new philosophical discipline, concerned with (a) the critical investigation of the conceptual nature and basic principles of information, including its dynamics (especially computation and information flow), utilization, and sciences, and with (b) the elaboration of information-theoretic and computational methodologies and their application to philosophical problems."

`r quote_footer('--- Floridi (2004) p. 555')`

Floridi proceeds to discuss the critical unanswered questions within the Philosophy of Information, starting with the need to define what information is [@floridi2004open]. Floridi (2004) then defines information as causative components of perceived reality not as a description of reality [@floridi2004open]. Then proceeding to use this perspective as the basis for his six hypothesis to define information, with a particular focus on communication theory`r margin_note('Much more work on the different options for the definition of Information is required')`.  The communication theory approach is derived from the work of Claude Shannon's 1948 mathematical model describing the reliable transmission of data across then telegraph wires and now the fiber optic world wide web[@shannon1948mathematical]. Within this primarily communication model, the definition of information then is the number of different data points possible to encode within a set size for a given data packet [@shannon1948mathematical].  This perspective sees data as being a physical entity without inherent meaning, hence can be either be right or wrong. Indeed, information, encoded as quantum particles, is seen as the data used in galactic scale processes analogous to terrestrial computers which drive the known universe [@stonier2012information]. Right and wrong then are whether the process both holts and develops a solution that is capable of sustaining itself within a given broader context.   Floridi, therefore, appears to be arguing that information as data underpins the computer-analogous process which creates an outcome and is irrelevant to the role or rightness of the answer to a given question [@floridi2004open].   

Floridi (2004) uses DNA as an example of information for reality. Apparently, this matches his vision as within the DNA is the genetic code for the consequent human, who is manifest by biological processes. The DNA code is a right explanation of the adult because it as the code was acted upon by a biological Turing machine a right human developed. Except it does not, a person's phenotype (the actual expression of the genetic map) does not equal their genotype (the genetic map), as some or many genes are not activated. So these latent genes stay hidden until manifest as a function of an external social process between humans, e.g. diet or smoking. Hence the phenotype does not guarantee the validity of the genotype to produce a "valid" human.

Simplistically  if I claim three birds are ducks, then fly them to another country, @shannon1948mathemtical would correctly argue that the transportation was successful if the same three birds arrived safely. He would not verify my claim that these were ducks on the basis of a successful transportation. 

`r newthought('Data science has a more mundane definition')` of data, information and knowledge.  In data science, the data elements are the atomic elements within a set of observations, the relationship between which provides information. Information serves as the basis for action [@anderson2015creating]. For example, whether the mumps, measles and rubella vaccination (MMR) status and the presence or absence of autism are atomic level data. Information is the relationship between the two. Namely, MMR does not cause autism. Knowledge for the parent is to set their mind at rest from the fraudulent process claiming there was a link [@brown2012uk]. The truth then rests on a familiar chain of support. Hume's realism on the observations, providing support for a valid theory as per Popper and the social support of others agreement to underpin individual action. Such a chain may not always be right, not following such a chain is always wrong except by chance.  

Floridi (2004) claims there has been a paradigm shift with the arrival of the information age, as part of his justification for the adoption of a new philosophical paradigm. An associated question is how much a change is required for the change to be a paradigm shift or alternatively has there been enough change? For scientists and mathematicians, the computer's arrival is a evolution of a well known tool set. Admitedly, one that can be applied on a previously unimaginable scale and speed. Yet this is not the paradigm shift such as found with Newton's laws of Physics or Einstein's special theory of relativity. There was life before these theories, and life after. Where life after was irreversibly different and there was no going back. It is entirely fitting that Newton and Einstein are household names. Yet Hollerith or Thompson are not known outside their field, and the impact of the mechanical tabulator or the Unix operation system with it's associated language C are of vast impact in the field of computer science, but only computer science. 

Like the steam engine in the late 19th and early 20th centuries, the impact of these new tools was again vast in sociological terms but not in the sense of a paradigm shift for scientists and philosophers changing their underlying paradigms. Increasing industrialisation had driven the move from the country to the cities, wars had reduced the overall workforce available so the capacity to feed the nation had dropped and the steam engine provided the means to increase food production at this time of need. Steam moved the goods to markets further away from the site of production, and together with steam on the farm changed the economics of farming. A sociological transformation for sure, but it did not change the science of the day, and was not regarded as a paradigm shift in philosophy especially epistemology. In the same sense there is no paradigm shift with the arrival of the computer, just the evolution into novel areas of use, participating in a sociological process that was already underway. 

`r newthought('@Cresswell1988 captures an alternative but equally familiar sense')` of information and how ordinary people make sense of meaning. Adopting Chomsky's differentiation between linguistic performance and semantic competence allows @Cresswell1988 to focus on the later. Yet this division echoes the common sense of language and meaning in society. There is a performance element of communicating in a given language, culture and geo-spatial point. And there is the need to understand the meaning conveyed by the words so as to be able to determine is the validity of any statement Cresswell1988].

Semantic competence`r margin_note('Validity is seen as being inherent to the semantic competence of a natural speaker considering the truth conditional of a claim in this and all possible worlds')` as seen in a natural speaker is an inherent property of language. The lowest common denominator of communication is speech, and the inability to speak excludes one from society more so than difficulties with reading or writing. Speech then is the most common way for information to be both passed between individuals and for judging the validity of the content.  @Cresswell1988 focuses on the central role of truth-conditions in understanding semantic competence, specifically the many worlds variety. So the truth-conditional validity needs to be considered not only in the current world but also in hypothetical other possible worlds [@Cresswell1988].  

The MMR example makes this formulation of validity easily accessible;

(@) A child received the MMR vaccination (_data_)
**and**
(@) Autism did not occur (_data_)
**hence**
(@) Immunisation is safe (_information_)
**and**
(@) I can _imagine_ a possible future world where my child gets the complications of Mumps, Measles or Rubella (_possible future state_)
**hence**
(@) I should give my child the MMR vaccination. (_knowledge_)

For @Cresswell1988 the process of a semantic understanding of the nature and validity and a claim is the same in the medieval marketplace as it is on today's internet. Yet there are fundamental differences in scale.

In the medieval marketplace I can only only talk or even hear a small number of people at a given time. The truth conditional nature of claims is either self evident (_"the harvest is bountiful"_) or irrelevant (_"Do you think the world is round?_"). I can only attend the market weekly or monthly.  The modern internet is a Turing machine; where others control the machine and the truth condition of data can be impossible to determine or is deliberately fraudulent. My social network of choice feeds to my smart watch 24 hours a day, and false narratives can be created to influence real world outcomes like national elections on a grand scale. 

#Summary
This is key argument. Assessing the validity of any claim has not inherently changed with the information age. The philosophical tools remain the same and understanding what it means to be right is a continuation of the past debate. 

What has changed is the scale of social interconnections that has engulfed the world and the abstraction of external reality, along with the deluge of random data points and information, spurious or otherwise. More work needs to occur in extending existing philosophical tools. There is no suggestion that philosophy of information is a new branch of philosophy, but it is an new lens on old debates.

\newpage
##Appendix 1: Prolog programming language
@colmerauer1996birth `r margin_note('Although logic is core to Prolog, the utility of Prolog in Logic is less certain. Currently this is a tool with potential but without philosophic purpose.')`describes both the birth and the intent of the Prolog computing language in the early 1970s.  Prolog's focus was to be in natural language processing, but with time Prolog has also developed to become a general purpose computing language (@liu2018logic). Its most common use remains in natural language processing and the development of expert systems including IBM Watson (@liu2018logic, p. 15). 

Prolog aims to enable the representation of knowledge through the used of formal predicate logic (@baral1994logic). Logic programming is unique among computing paradigms in that it includes the concepts of fact and validity of expressions as fundamental core constructs, inherent to the language itself (@colmerauer1996birth). 

To a logistician, Prolog is an idiosyncratic representation of the usual and commonplace. Consider modus pons;

$$p \supset q$$
$$p$$
**Hence**
$$q$$

is represented in Prolog as;

\begin{center}
p \newline
q :- p \newline
\end{center}

Which is to be read as p is true, hence q must be true given p is true.

So Prolog provides a vehicle for knowledge representation through the use of formal logic. What is less obvious is what it adds to a discussion of the logic itself and its underlying formal proof.

\newpage
