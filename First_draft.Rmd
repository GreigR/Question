---
title: "Being right in the information era"
subtitle: "First draft for discussion"
author: "Greig Russell"
date: "`r Sys.Date()`"
output:
  tufte::tufte_handout:
    citation_package: natbib
    latex_engine: xelatex
  tufte::tufte_html: default
  tufte::tufte_book:
    citation_package: natbib
    latex_engine: xelatex
bibliography: Reference.bib
link-citations: yes
---



```{r setup, include=FALSE}
library(tufte)
# invalidate cache when the tufte version changes
knitr::opts_chunk$set(tidy = FALSE, cache.extra = packageVersion('tufte'))
options(htmltools.dir.version = FALSE)
```

\pagenumbering{arabic}

# Introduction

`r newthought('No question occurs without a context')` or an origin story. Typically, in philosophy, questions like "What is the answer to " are immediately followed by "how do I know that I am right"? This thesis will focus on the epistemological challenge of the later. This issue has been the focus of philosophers (Descartes) and non-philosophers (Trump) for many centuries.  

`r newthought('Descartes [1641]')` considered the question of the nature of his reality by denying the relevance of sense data and only believing in the reality of his cognition. Arguing that he could be asleep and in a dream without the ability to distinguish this state from waking and perceiving, so only his thoughts were undisputably real. This lead to the famous position of "I think therefore I am" as the basis for his philosophical meditations on the nature of his reality [Descartes1641].  

In Meditation four,`r margin_note('Descrates argues he is right when his beliefs and actions are aligned to an external source of truth, in his case the Judo-Christian God as perceived in the 15th Century')` @Descartes1641 uses as his truth determinant the stated existence of God, who inherently does not have properties which would render his _(sic)_  judgement open to any doubt. Descartes, on the other hand, states his imperfections as being the basis of his errors [Descartes1641].    

Error or falsity occurs when Descartes exercises his free will (actions) to move beyond his cognitive understanding of a given situation. Conversely, if Descartes confines his efforts to situations where he has full knowledge, then he is right as evidenced by his ability to "so see there the image of God [Descartes1641]."  

 `r newthought('Hume [1751]')` had the diametrically opposite perspective to that of Descartes. Primary sensory perceptions and the impressions generated from reflections on those perceptions are the basis of ideas [@Hume1751, Section II - Of the origin of ideas]. 

The association between ideas for Hume can be one of "Resemblance" or "Continuity" and most importantly "Cause and effect" [@Hume1751, Section III - Of the association of ideas]. The first two can never have truth validity associated with them. A Tuatara resembles a lizard; only it is not. All swans are not white, but all swans in Europe are white, so when a European is thinking of a swan they will recall only a white swan, but this does not prove all swans are white.

In Section IV `r margin_note('Hume argues for the primacy of the senses as the basis of ideas. For Hume being right is establishing a causative relationship is an internally consistent chain that invariably decsribes the connection and is based on other sense derived phenomea or basic matehmatical axioms')`- "Sceptical doubts concerning the operations of the understanding" @Hume1759 focuses on the _"is-ought"_ paradox he first raised in his "A Treatise on Human Understanding" [@Hume1739, p. 379]. Namely, because one thing **is** associated with another,  does not mean they **ought** to be associated with each other.  For Hume, such infinite scepticism will be addressed through human reason, which he divides into two categories; "Relations of ideas" and "Matters of fact" [-@Hume1751].  

The concept of "Relations of ideas" is obviously influenced by Newton's laws of Physics, where describing the causal relationship between two ideas is mathematically derived and underpinned by the self-evident core axioms [@Hume1751, Part 1, Section IV]. Where the force created in one billard ball is transfered to another by the collision and described by the formula for Newton's second law;

$$Force = Mass. Acceleration$$

"Matters of fact" or cause and effect between two ideas have to transcend the temporal or spatial. The example used is free will, or human thought can control finger movement but not liver action, so free will cannot cause the finger to move as it is not necessary or sufficient. For Hume, finger movement derives from the nerves connecting the brain to the finger and arm muscles. Each component in this causal chain is a sensory derived idea [@Hume1751, Part 1, Section VII - Of the idea of necessary connection].  

In this Hume was describing a deterministic universe based on internal consistency and derived from sense based information. Excluded are concepts of cognition or external social sources of validity including theism. These excludions were fundamental to @Descartes1641 formula for being correct.  

`r newthought("History plays cruel jests")` on the wise. @Hume1751 rejected probability as a sufficient basis to underpin determination of cause behind an effect. The relentless determinism which followed instead demonstrated that the universe was by nature inherently probabilistic. 

In the early 20th century `r margin_note('Statistical truth merges the concepts of an intrinsic casual relationship defined by mathematics derived from fundamental axioms')`, statistics and probability developed into cornerstones of modern science, so that now little-deemed science occurs without them, and statistical techniques have spilt into non-traditional subjects such literature through techniques like text analysis [@silge2016tidytext].

@fisher1925statistical describes how he saw statistics as being the application of applied mathematics to populations. In his work, this original concept had been extended to an understanding of variation and the "reduction of data" (@fisher1925statistical p. 1). His worked extended into the study of frequency distributions, which describe the spread of a characteristic within a population and enable the use of specific statistical models to explain that variation [@fisher1925statistical]. 

Described is also the work by Pearson on correlation or covariance (@fisher1925statistical, p.6). Covariance describes how the stronger two properties of a population are related can be measured by how much they vary at the same time.  @student1908probable describes the differences between two means, where a mean is a measure of the central tendency within a population. 

As a result of this work, the truth or validity was about the properties of two related populations; whether the difference was mathematically higher than that expected by chance alone or conversely if these two populations varied were statistically covariant then it could mean they were causally related. This new methodology not only suggested possible avenues for further study, but it also provided a vehicle to demonstrate which explanation is most likely [@field2012discovering].

`r newthought('The philosophy of science')` presents its student with a different aspect to the challenge of knowing if you are right.`r margin_note('For Popper being right was having a valid thory that now one had proved wrong')` As most scientific theories are wrong and only a stepping stone towards a deeper understanding of the underlying question. For @popper2005logic the answer was that scientific theories were right if no one has proved them wrong. The nuance of this seemingly self-evident statement was for a scientific theory to be a valid theory; it had to generate new hypotheses to test including at least one that if true would disprove the theory [@popper2005logic].

For @kuhn2012structure, Popper may have described a scientific theory and its exploration through progressive experimentation, but there was more to science. `r margin_note('Kuhn extended this theory of being right to include a social acceptance by the scientific community')` For Kuhn saw the process of science as being organised around a central paradigm (-@kuhn2012structure).  Where adoption of this theory was a social process, as was its abandonment. Where doubts about the incumbent rose with evidence that did falsify it, leading to the development of the new theory. Adoption of this new theory would be a social process across the sictentific community as a whole [@kuhn2012structure].

#The information era
`r newthought('A necessary precursor')` to any information age was the development of the modern computer.`r margin_note('For Turing the computer assisted the expert, the source of truth and the validity of the answer was the user and not the computer')`  Turing (1937) conceptualization has become the core design concept, of which everything since has only been an implementation.  The components of his machine were;

1. A paper tape, which is usually described as being infinite, which contains the a set of 1's and 0's. The initial state of this paper tape describes the input to the process, the final state is the output.
2. The head which reads the paper tape and consults the set of instructions as to what next steps to take.
3. A set of instructions, such as change the value of this cell, then move n places to the left.

For Turing the machine was mostly a means to an end as the focus on his work was solving the "Holting problem", namely would such machines always end, with the answer being "no they would not" [@turing1937computable].  

There are two key elements to the Turing machine. The first is that it has no concept of validity. The message on the tape is what it was, both initially and on successful termination of what we would now call the program. Instead it used identity as a substitute. I would argue this is not an overisght, but a manifestation of Turing's reality. He was the source of validity and truth. The computer assisted him in that process, but was not in itself ever meant to be a source of the truth.

![A slide rule](/home/greig/Philosophy/Question/Slide_rule.png)

Consider a more accessible example the slide riule, the pocket calculator for all scientists and mathematicians of the era. Usefully the manufacturer places a mark for $\pi$ at 3.14. Slide rule users of ear would all know the value of $\pi$ and this was only a convenience, not an indication of the truth as to the value of $\pi$. If the mark was found to be in the wrong place, it would not cause error as the source of validity was with the user. 


\newpage
##Appendix 1: Prolog programming language
@colmerauer1996birth `r margin_note('Although logic is core to Prolog, the utility of Prolog in Logic is less certain. Currently this is a tool with potential but without purpose.')`describes both the birth and the intent of the Prolog computing language in the early 1970s.  Prolog's focus was to be in natural language processing, but with time Prolog has also developed to become a general purpose computing language (@liu2018logic). Its most common use remains in natural language processing and the development of expert systems including IBM Watson (@liu2018logic, p. 15). 

Prolog aims to enable the representation of knowledge through the used of formal predicate logic (@baral1994logic). Logic programming is unique among computing paradigms in that it includes the concepts of fact and validity of expressions as fundamental core constructs, inherent to the language itself (@colmerauer1996birth). 

To a logistician, Prolog is an idiosyncratic representation of the usual and commonplace. Consider modus pons;

$$p \supset q$$
$$p$$
**Hence**
$$q$$

is represented in Prolog as;

\begin{center}
p \newline
q :- p \newline
\end{center}

Which is to be read as p is true, hence q must be true given p is true.

So Prolog provides a vehicle for knowledge representation through the use of formal logic. What is less obvious is what it adds to a discussion of the logic itself and its underlying formal proof.

\newpage